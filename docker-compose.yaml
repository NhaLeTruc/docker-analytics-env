######################################################################################
### Iceberg - MinIO - Nessie Catalog - Spark - JupyterLab - SuperSet - DqOps Setup ###
######################################################################################

version: '3'
# include:
#   - path: 'docker-compose.main.yaml'
#   - path: 'docker-compose.airflow.yaml'
services:
  ###########
  ## MinIO ##
  ###########
  minio-lake:
    image: bitnami/minio:2025.6.13-debian-12-r0
    command: ["server", "/data", "--console-address", ":9001"]
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Web console
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - minio-data:/data
    entrypoint: >
      /bin/sh -c "
      minio server /data --console-address ':9001' &
      sleep 5;
      mc alias set myminio http://localhost:9000 ${MINIO_USER_USERNAME} ${MINIO_USER_PASSWORD};
      mc admin accesskey create myminio/ --access-key ${MINIO_ACCESS_KEY} --secret-key ${MINIO_SECRET_KEY} --description 'Access key for docker environment';
      mc mb myminio/dummy;
      mc cp -r /data_seeder/dummy/* myminio/dummy/;
      tail -f /dev/null"
    networks:
      - lakehouse_network

  ##########################
  ## Spark and JupyterLab ##
  ##########################
  spark-master:
    image: bitnami/spark:3.5.6-debian-12-r0
    command: bin/spark-class org.apache.spark.deploy.master.Master
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    ports:
      - 8080:8080
      - 7077:7077
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./app:/app # persistent storage
    networks:
      - lakehouse_network

  spark-worker-1:
    image: bitnami/spark:3.5.6-debian-12-r0
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./app:/app # persistent storage
    networks:
      - lakehouse_network

  spark-worker-2:
    image: bitnami/spark:3.5.6-debian-12-r0
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./app:/app # persistent storage
    networks:
      - lakehouse_network

  jupyterlab:
    depends_on:
      - spark-master
    build: ./spark/Dockerfile.jupyterlab
    command: python -m jupyterlab --ip "0.0.0.0" --no-browser --NotebookApp.token=''
    ports:
      - 8888:8888
    volumes:
      - ./app:/app # persistent storage
    environment:
      - JUPYTER_ENABLE_LAB=yes
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8888" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - lakehouse_network
  
  spark-thrift:
    image: bitnami/spark:3.5.6-debian-12-r0
    depends_on:
      spark-master:
        condition: service_started
    container_name: spark-thrift
    command: [
      "/opt/bitnami/spark/sbin/start-thriftserver.sh",
      "--conf", "spark.cores.max=2"
    ]
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    ports:
      - "10000:10000"
    volumes:
      # - ./airflow/data:/opt/airflow/data
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./dqops_userhome/.data/check_results:/data/check_results
    networks:
      - lakehouse_network

  ####################
  ## Nessie Catalog ##
  ####################
  nessie:
    image: ghcr.io/projectnessie/nessie:0.83.1
    container_name: nessie
    ports:
      - "19120:19120"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - QUARKUS_PROFILE=prod
      - QUARKUS_HTTP_PORT=19120
      - NESSIE_VERSION_STORE_TYPE=JDBC
      - QUARKUS_DATASOURCE_JDBC_URL=${DB_JDBC_URI}
      - QUARKUS_DATASOURCE_USERNAME=${DB_USERNAME}
      - QUARKUS_DATASOURCE_PASSWORD=${DB_PASSWORD}
      - QUARKUS_LOG_LEVEL=INFO
    networks:
      - lakehouse_network

  #################################################
  ## Postgres DB for superset and nessie backend ##
  #################################################
  postgres:
    image: postgres:16
    restart: always
    env_file: .env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - superset_db_data:/var/lib/postgresql/data/superset/
      - nessie_db_data:/var/lib/postgresql/data/nessie/
    networks:
      - lakehouse_network

  pgadmin:
    image: dpage/pgadmin4:latest
    env_file: .env
    ports:
      - "5050:80"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "80"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s      
    volumes:
      - pgadata:/var/lib/pgadmin
      - ./pgadmin/servers.json:/pgadmin4/servers.json:ro
    networks:
      - lakehouse_network

  ###################################
  ## Superset and Data Quality OPs ##
  ###################################
  superset:
    image: apache/superset:latest
    build:
      context: ./superset
    container_name: superset
    env_file: .env
    environment:
      SQLALCHEMY_DATABASE_URI: >-
        postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@superset_db:5432/${POSTGRES_DB}
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
    volumes:
      - superset_data:/app/superset_home
      - ./superset/superset_config.py:/app/superset_config.py
    ports:
      - "8088:8088"
    depends_on:
      db:
        condition: service_healthy
    networks:
      - lakehouse_network

  dqops:
    image: dqops/dqo
    container_name: dqops
    command: ${DQO_CMD}
    ports:
      - "8888:8888"
    volumes:
      - ./dqops_userhome:/dqo/userhome
    networks:
      - lakehouse_network

networks:
  lakehouse_network:
    driver: bridge

volumes:
  minio-data:
  spark-logs:
  superset_db_data:
  nessie_db_data:

