######################################################################################
### Iceberg - MinIO - Nessie Catalog - Spark - JupyterLab - SuperSet - DqOps Setup ###
######################################################################################

version: '3'

services:
  ###########
  ## MinIO ##
  ###########
  minio-lake:
    image: bitnami/minio:2025.6.13-debian-12-r0
    container_name: minio-lake
    command: ["server", "/data", "--console-address", ":9001"]
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Web console
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - minio-data:/bitnami/minio/data
      - ./dummy:/bitnami/minio/dummy      
    entrypoint: >
      /bin/sh -c "
      minio server /bitnami/minio/data --console-address ':9001' &
      sleep 5;
      mc alias set myminio http://localhost:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      mc admin accesskey create myminio/ --access-key ${MINIO_ACCESS_KEY} --secret-key ${MINIO_SECRET_KEY} --description 'Access key for docker environment';
      mc mb myminio/dummy;
      mc mb myminio/datalake;
      mc mb myminio/datalakehouse;
      mc mb myminio/warehouse;
      mc cp -r /dummy/* myminio/dummy/;
      tail -f /dev/null"
    networks:
      - lakehouse_network

  ##########################
  ## Spark and JupyterLab ##
  ##########################
  spark-master:
    image: bitnami/spark:3.5.6-debian-12-r0
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    ports:
      - 8080:8080
      - 7077:7077
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./app:/app # persistent storage
    networks:
      - lakehouse_network

  spark-worker-1:
    image: bitnami/spark:3.5.6-debian-12-r0
    container_name: spark-worker-1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./app:/app # persistent storage
    networks:
      - lakehouse_network

  spark-worker-2:
    image: bitnami/spark:3.5.6-debian-12-r0
    container_name: spark-worker-2
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./app:/app # persistent storage
    networks:
      - lakehouse_network

  jupyterlab:
    container_name: jupyterlab
    depends_on:
      - spark-master
    build: 
      context: ./spark
      dockerfile: Dockerfile.jupyterlab
    command: python -m jupyterlab --ip "0.0.0.0" --no-browser --NotebookApp.token=''
    ports:
      - 8888:8888
    volumes:
      - ./app:/app # persistent storage
    environment:
      - JUPYTER_ENABLE_LAB=yes
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8888" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - lakehouse_network
  
  spark-thrift:
    image: bitnami/spark:3.5.6-debian-12-r0
    container_name: spark-thrift
    depends_on:
      spark-master:
        condition: service_started
    command: [
      "/opt/bitnami/spark/sbin/start-thriftserver.sh",
      "--conf", "spark.cores.max=2"
    ]
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    ports:
      - "10000:10000"
    volumes:
      # - ./airflow/data:/opt/airflow/data
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./dqops_userhome/.data/check_results:/data/check_results
    networks:
      - lakehouse_network

  ####################
  ## Nessie Catalog ##
  ####################
  nessie:
    image: ghcr.io/projectnessie/nessie:latest
    container_name: nessie-catalog
    env_file: .env
    ports:
      - "19120:19120"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - QUARKUS_PROFILE=prod
      - QUARKUS_HTTP_PORT=19120
      - NESSIE_VERSION_STORE_TYPE=JDBC
      - QUARKUS_DATASOURCE_JDBC_URL=jdbc:postgresql://postgres-metadata:5432/${POSTGRES_DB}
      - QUARKUS_DATASOURCE_USERNAME=${POSTGRES_USER}
      - QUARKUS_DATASOURCE_PASSWORD=${POSTGRES_PASSWORD}
      - QUARKUS_LOG_CONSOLE_FORMAT=%d{yyyy-MM-dd HH:mm:ss} %-5p [%c{1.}] (%t) %s%e%n
      - QUARKUS_LOG_LEVEL=INFO
    networks:
      - lakehouse_network

  #################################################
  ## Postgres DB for superset and nessie backend ##
  #################################################
  postgres:
    image: postgres:16
    container_name: postgres-metadata
    restart: always
    env_file: .env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - superset_db_data:/var/lib/postgresql/data/
      - nessie_db_data:/var/lib/postgresql/data/
    networks:
      - lakehouse_network

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin4
    env_file: .env
    ports:
      - "5050:80"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "80"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s      
    volumes:
      - pgadata:/var/lib/pgadmin
      - ./pgadmin/servers.json:/pgadmin4/servers.json:ro
    networks:
      - lakehouse_network

  ###################################
  ## Superset and Data Quality OPs ##
  ###################################
  superset:
    container_name: superset
    build:
      context: ./superset
      dockerfile: Dockerfile
    environment:
      SQLALCHEMY_DATABASE_URI: >-
        postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres-metadata:5432/${POSTGRES_DB}
      SUPERSET_LOAD_EXAMPLES: "yes"
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}      
    volumes:
      - superset_data:/app/superset_home
      - ./superset/superset_config.py:/app/superset_config.py
    ports:
      - "8088:8088"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - lakehouse_network

  dqops:
    image: dqops/dqo
    container_name: data-quality-ops
    command: ${DQO_CMD}
    ports:
      - "8889:8888"
    volumes:
      - ./dqops_userhome:/dqo/userhome
    networks:
      - lakehouse_network

networks:
  lakehouse_network:
    driver: bridge

volumes:
  minio-data:
  spark-logs:
  pgadata:
  superset_data:
  superset_db_data:
  nessie_db_data:

