# spark configs
spark.master                            spark://spark-master:7077
spark.jars.packages                     org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:2.12,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:2.12
# minio
spark.hadoop.fs.s3a.endpoint            http://minio-lake:9000
spark.hadoop.fs.s3a.path.style.access   true
spark.hadoop.fs.s3a.impl                org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.access.key          minioadmin
# spark.hadoop.fs.s3a.secret.key          minioadmin

# spark logs
spark.eventLog.enabled                 true
spark.eventLog.dir                     /tmp/spark-events
spark.history.fs.logDirectory          /tmp/spark-events

# Iceberg and Nessie
spark.sql.extensions                   org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
spark.sql.catalog.data                 org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.data.warehouse       s3://myminio/datalakehouse
spark.sql.catalog.data.s3.endpoint	   http://minio-lake:9000
spark.sql.catalog.data.io-impl		   org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.data.catalog-impl    org.apache.iceberg.nessie.NessieCatalog
spark.sql.catalog.data.uri             http://nessie:19120/api/v1
spark.sql.catalog.data.ref             main
spark.sql.catalog.data.authentication.type  NONE
# spark.sql.catalog.data.authentication.type  BEARER
# spark.sql.catalog.data.authentication.token XXXXX-XXXXX-XXXXX
spark.sql.defaultCatalog               data
# spark.sql.catalogImplementation        in-memory
# spark.sql.catalogImplementation        hive

spark.sql.execution.pyarrow.enabled    true

# ======================================================================================
# Performance Tuning Configurations
# ======================================================================================
#
# These settings are configured to maximize resource utilization for faster processing.
#
# -- Resource Allocation ---------------------------------------------------------------
# Use dynamic allocation to let Spark scale executors based on workload.
spark.dynamicAllocation.enabled                 true
spark.dynamicAllocation.shuffleTracking.enabled true
spark.dynamicAllocation.minExecutors            1
spark.dynamicAllocation.maxExecutors            10
spark.dynamicAllocation.initialExecutors        2

# -- General Execution -----------------------------------------------------------------
spark.cores.max                     8

# -- Memory Settings -------------------------------------------------------------------
spark.driver.memory                 8g
spark.executor.memory               8g
spark.executor.memoryOverhead       1g


